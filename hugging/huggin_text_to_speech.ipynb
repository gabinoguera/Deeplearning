{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-05T13:31:28.479109700Z",
     "start_time": "2024-06-05T13:31:24.333965900Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "narrator = pipeline(\"text-to-speech\",\n",
    "                    model=\"./models/kakao-enterprise/vits-ljs\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T13:31:54.708851300Z",
     "start_time": "2024-06-05T13:31:30.504005Z"
    }
   },
   "id": "9275f3f1c90ae5ad"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Researchers at the Allen Institute for AI, \\\n",
    "HuggingFace, Microsoft, the University of Washington, \\\n",
    "Carnegie Mellon University, and the Hebrew University of \\\n",
    "Jerusalem developed a tool that measures atmospheric \\\n",
    "carbon emitted by cloud servers while training machine \\\n",
    "learning models. After a model’s size, the biggest variables \\\n",
    "were the server’s location and time of day it was active.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T13:33:15.423135400Z",
     "start_time": "2024-06-05T13:33:15.413130200Z"
    }
   },
   "id": "41fa759965f21528"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "espeak not installed on your system",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m narrated_text \u001B[38;5;241m=\u001B[39m \u001B[43mnarrator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\pipelines\\text_to_audio.py:182\u001B[0m, in \u001B[0;36mTextToAudioPipeline.__call__\u001B[1;34m(self, text_inputs, **forward_params)\u001B[0m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_inputs: Union[\u001B[38;5;28mstr\u001B[39m, List[\u001B[38;5;28mstr\u001B[39m]], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params):\n\u001B[0;32m    161\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m    Generates speech/audio from the inputs. See the [`TextToAudioPipeline`] documentation for more information.\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;124;03m        - **sampling_rate** (`int`) -- The sampling rate of the generated audio waveform.\u001B[39;00m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(text_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1243\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[0;32m   1236\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[0;32m   1237\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1240\u001B[0m         )\n\u001B[0;32m   1241\u001B[0m     )\n\u001B[0;32m   1242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1249\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[0;32m   1248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m-> 1249\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m   1250\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\pipelines\\text_to_audio.py:126\u001B[0m, in \u001B[0;36mTextToAudioPipeline.preprocess\u001B[1;34m(self, text, **kwargs)\u001B[0m\n\u001B[0;32m    122\u001B[0m     new_kwargs\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[0;32m    124\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m new_kwargs\n\u001B[1;32m--> 126\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer(text, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2883\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2881\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[0;32m   2882\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[1;32m-> 2883\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_one(text\u001B[38;5;241m=\u001B[39mtext, text_pair\u001B[38;5;241m=\u001B[39mtext_pair, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mall_kwargs)\n\u001B[0;32m   2884\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2885\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2969\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2964\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2965\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2966\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2967\u001B[0m         )\n\u001B[0;32m   2968\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[1;32m-> 2969\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[0;32m   2970\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[0;32m   2971\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   2972\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m   2973\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   2974\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m   2975\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m   2976\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[0;32m   2977\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m   2978\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m   2979\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m   2980\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m   2981\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m   2982\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m   2983\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m   2984\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m   2985\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m   2986\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2987\u001B[0m     )\n\u001B[0;32m   2988\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2989\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[0;32m   2990\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[0;32m   2991\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3007\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3008\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3160\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   3150\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m   3151\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[0;32m   3152\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m   3153\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3157\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3158\u001B[0m )\n\u001B[1;32m-> 3160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[0;32m   3161\u001B[0m     batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[0;32m   3162\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   3163\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[0;32m   3164\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m   3165\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m   3166\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m   3167\u001B[0m     is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[0;32m   3168\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m   3169\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m   3170\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m   3171\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m   3172\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m   3173\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m   3174\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m   3175\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m   3176\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m   3177\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3178\u001B[0m )\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\tokenization_utils.py:803\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._batch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    801\u001B[0m     ids, pair_ids \u001B[38;5;241m=\u001B[39m ids_or_pair_ids\n\u001B[1;32m--> 803\u001B[0m first_ids \u001B[38;5;241m=\u001B[39m \u001B[43mget_input_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m second_ids \u001B[38;5;241m=\u001B[39m get_input_ids(pair_ids) \u001B[38;5;28;01mif\u001B[39;00m pair_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    805\u001B[0m input_ids\u001B[38;5;241m.\u001B[39mappend((first_ids, second_ids))\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\tokenization_utils.py:770\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_input_ids\u001B[39m(text):\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 770\u001B[0m         tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenize(text, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    771\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(tokens)\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\tokenization_utils.py:559\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.tokenize\u001B[1;34m(self, text, **kwargs)\u001B[0m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    543\u001B[0m \u001B[38;5;124;03mConverts a string into a sequence of tokens, using the tokenizer.\u001B[39;00m\n\u001B[0;32m    544\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    555\u001B[0m \u001B[38;5;124;03m    `List[str]`: The list of tokens.\u001B[39;00m\n\u001B[0;32m    556\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    557\u001B[0m split_special_tokens \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit_special_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_special_tokens)\n\u001B[1;32m--> 559\u001B[0m text, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_for_tokenization(text, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[0;32m    562\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKeyword arguments \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwargs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not recognized.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\transformers\\models\\vits\\tokenization_vits.py:186\u001B[0m, in \u001B[0;36mVitsTokenizer.prepare_for_tokenization\u001B[1;34m(self, text, is_split_into_words, normalize, **kwargs)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_phonemizer_available():\n\u001B[0;32m    184\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install the `phonemizer` Python package to use this tokenizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 186\u001B[0m     filtered_text \u001B[38;5;241m=\u001B[39m \u001B[43mphonemizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphonemize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiltered_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men-us\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mespeak\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstrip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreserve_punctuation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwith_stress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    194\u001B[0m     filtered_text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms+\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, filtered_text)\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m normalize:\n\u001B[0;32m    196\u001B[0m     \u001B[38;5;66;03m# strip any chars outside of the vocab (punctuation)\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\phonemizer\\phonemize.py:206\u001B[0m, in \u001B[0;36mphonemize\u001B[1;34m(text, language, backend, separator, strip, prepend_text, preserve_empty_lines, preserve_punctuation, punctuation_marks, with_stress, tie, language_switch, words_mismatch, njobs, logger)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;66;03m# initialize the phonemization backend\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m backend \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mespeak\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 206\u001B[0m     phonemizer \u001B[38;5;241m=\u001B[39m \u001B[43mBACKENDS\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpunctuation_marks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpunctuation_marks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreserve_punctuation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreserve_punctuation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwith_stress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_stress\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtie\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtie\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlanguage_switch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlanguage_switch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwords_mismatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwords_mismatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mespeak-mbrola\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    216\u001B[0m     phonemizer \u001B[38;5;241m=\u001B[39m BACKENDS[backend](\n\u001B[0;32m    217\u001B[0m         language,\n\u001B[0;32m    218\u001B[0m         logger\u001B[38;5;241m=\u001B[39mlogger)\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\phonemizer\\backend\\espeak\\espeak.py:45\u001B[0m, in \u001B[0;36mEspeakBackend.__init__\u001B[1;34m(self, language, punctuation_marks, preserve_punctuation, with_stress, tie, language_switch, words_mismatch, logger)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, language: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     38\u001B[0m              punctuation_marks: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Pattern]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     39\u001B[0m              preserve_punctuation: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m              words_mismatch: WordMismatch \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     44\u001B[0m              logger: Optional[Logger] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 45\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpunctuation_marks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpunctuation_marks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreserve_punctuation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreserve_punctuation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_espeak\u001B[38;5;241m.\u001B[39mset_voice(language)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_stress \u001B[38;5;241m=\u001B[39m with_stress\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\phonemizer\\backend\\espeak\\base.py:39\u001B[0m, in \u001B[0;36mBaseEspeakBackend.__init__\u001B[1;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, language: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     36\u001B[0m              punctuation_marks: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Pattern]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     37\u001B[0m              preserve_punctuation: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     38\u001B[0m              logger: Optional[Logger] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpunctuation_marks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpunctuation_marks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreserve_punctuation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreserve_punctuation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_espeak \u001B[38;5;241m=\u001B[39m EspeakWrapper()\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloaded \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_espeak\u001B[38;5;241m.\u001B[39mlibrary_path)\n",
      "File \u001B[1;32m~\\Desktop\\papeles personales\\Deeplearning\\hugging\\env\\lib\\site-packages\\phonemizer\\backend\\base.py:77\u001B[0m, in \u001B[0;36mBaseBackend.__init__\u001B[1;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# ensure the backend is installed on the system\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m---> 77\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(  \u001B[38;5;66;03m# pragma: nocover\u001B[39;00m\n\u001B[0;32m     78\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m not installed on your system\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname()))\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logger \u001B[38;5;241m=\u001B[39m logger\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minitializing backend \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mversion()))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: espeak not installed on your system"
     ]
    }
   ],
   "source": [
    "narrated_text = narrator(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T13:33:17.188081200Z",
     "start_time": "2024-06-05T13:33:16.179588700Z"
    }
   },
   "id": "3ba127afd394dc22"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'narrated_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Audio \u001B[38;5;28;01mas\u001B[39;00m IPythonAudio\n\u001B[1;32m----> 3\u001B[0m IPythonAudio(\u001B[43mnarrated_text\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m      4\u001B[0m              rate\u001B[38;5;241m=\u001B[39mnarrated_text[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msampling_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'narrated_text' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio as IPythonAudio\n",
    "\n",
    "IPythonAudio(narrated_text[\"audio\"][0],\n",
    "             rate=narrated_text[\"sampling_rate\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T13:33:23.519746800Z",
     "start_time": "2024-06-05T13:33:23.477945400Z"
    }
   },
   "id": "404249860b535a70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f60944bbf31c4105"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
